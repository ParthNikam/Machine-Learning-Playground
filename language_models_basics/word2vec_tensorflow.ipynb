{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011092e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c769c1e",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "\n",
    "We'll be using Skip-Gram for this example. <br/>\n",
    "The objective of skip-gram is to maximize the probability of predicting the context of the given target word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa4187f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12be0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let tokenize this sentence \n",
    "sentence = \"The quick brown fox jumps over the lazy dogs\"\n",
    "tokens = list(sentence.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee52212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 {'<pad>': 0, 'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dogs': 8}\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary to save the mappings from tokens to their integer indices\n",
    "\n",
    "vocab = {}\n",
    "index = 1\n",
    "vocab['<pad>'] = 0 # add a padding token\n",
    "\n",
    "for token in tokens:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ea6223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 5], [2, 4], [5, 1], [1, 5], [5, 4], [5, 6], [3, 1], [6, 1], [7, 1], [7, 8], [2, 1], [8, 7], [6, 7], [6, 4], [3, 5], [1, 7], [5, 3], [1, 2], [2, 3], [7, 6], [1, 6], [1, 3], [4, 2], [3, 4], [4, 6], [3, 2], [8, 1], [1, 8], [6, 5], [4, 3]]\n",
      "30\n",
      "(4, 5): (jumps, over)\n",
      "(2, 4): (brown, jumps)\n",
      "(5, 1): (over, quick)\n",
      "(1, 5): (quick, over)\n",
      "(5, 4): (over, jumps)\n"
     ]
    }
   ],
   "source": [
    "# generate skip-grams for one sentence\n",
    "# skip grams are basically combinations of target word and surrounding words in the sliding window \n",
    "\n",
    "example_sequence = [vocab[word] for word in tokens]\n",
    "window_size = 2\n",
    "\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    example_sequence, vocabulary_size=vocab_size, window_size=window_size, negative_samples=0\n",
    ")\n",
    "\n",
    "print(positive_skip_grams)\n",
    "print(len(positive_skip_grams))\n",
    "\n",
    "for target, context in positive_skip_grams[:5]:\n",
    "    print(f\"({target}, {context}): ({tokens[target]}, {tokens[context]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8861ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "with open(path_to_file) as f:\n",
    "  lines = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ffde2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422e6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize sentences from the corpus\n",
    "\n",
    "# custom standardizing function to convert text to lowercase and remove punctuations\n",
    "def custom_std(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_std,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba55d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c104ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f521f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32777\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83eeaf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
      "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
      "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
      "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "760e36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd458cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32777/32777 [00:17<00:00, 1917.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (65176,)\n",
      "contexts.shape: (65176, 5)\n",
      "labels.shape: (65176, 5)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca91d6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# the tf.data.Dataset helps to operform efficient batching for large number of training examples\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a65c0a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7784377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf82e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0933a0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.6082 - accuracy: 0.2293\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5884 - accuracy: 0.5527\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.5399 - accuracy: 0.5960\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.4571 - accuracy: 0.5696\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.3594 - accuracy: 0.5770\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.2625 - accuracy: 0.6054\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.1719 - accuracy: 0.6413\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0879 - accuracy: 0.6759\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.0099 - accuracy: 0.7088\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.9375 - accuracy: 0.7380\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.8705 - accuracy: 0.7643\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.8085 - accuracy: 0.7863\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.7514 - accuracy: 0.8058\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.6990 - accuracy: 0.8233\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.6511 - accuracy: 0.8377\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.6074 - accuracy: 0.8519\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.5675 - accuracy: 0.8647\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.5312 - accuracy: 0.8764\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.4982 - accuracy: 0.8863\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.4681 - accuracy: 0.8948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x169ba9da920>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam', \n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "\n",
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4dabdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43c315bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the learned embedding matrix (target side)\n",
    "embedding_matrix = word2vec.target_embedding.weights[0].numpy()   # shape: (vocab_size, embedding_dim)\n",
    "\n",
    "\n",
    "# map word to index\n",
    "word_to_id = {w: i for i, w in enumerate(inverse_vocab)}   # `inverse_vocab` from the vectorizer\n",
    "id_to_word = {i: w for w, i in word_to_id.items()}\n",
    "\n",
    "def get_embedding(word: str) -> np.ndarray:\n",
    "    \"\"\"Return the embedding vector for `word` (or raise if OOV).\"\"\"\n",
    "    idx = word_to_id.get(word)\n",
    "    if idx is None:\n",
    "        raise ValueError(f\"'{word}' not in vocabulary\")\n",
    "    return embedding_matrix[idx]\n",
    "\n",
    "\n",
    "# Find top-k similar words (cosine similarity)\n",
    "def most_similar(query: str, top_k: int = 5):\n",
    "    query_vec = get_embedding(query).reshape(1, -1)\n",
    "    sims = cosine_similarity(query_vec, embedding_matrix)[0]          # similarity to every vocab entry\n",
    "    # Exclude the query word itself\n",
    "    sims[word_to_id[query]] = -np.inf\n",
    "    top_ids = np.argsort(sims)[-top_k:][::-1]                        # descending order\n",
    "    return [(id_to_word[i], sims[i]) for i in top_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f459410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      excuse  (cosine ≈ 0.5144)\n",
      "    shepherd  (cosine ≈ 0.4980)\n",
      "    marrying  (cosine ≈ 0.4657)\n",
      "   authority  (cosine ≈ 0.4626)\n",
      "   signories  (cosine ≈ 0.4623)\n"
     ]
    }
   ],
   "source": [
    "word = \"heart\"\n",
    "for w, score in most_similar(word):\n",
    "    print(f\"{w:>12}  (cosine ≈ {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8776a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
