{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162d1223",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1ccface",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d73471",
   "metadata": {},
   "source": [
    "### N gram vectors of text sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b2ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a1e2698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 1 1 0 0 0 1 1 0]\n",
      " [0 0 1 0 0 0 0 1 0 1 1 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 2 0 0 0 0 0]\n",
      " [0 1 0 1 0 1 0 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# bag of words model\n",
    "\n",
    "# text preprocessing: \n",
    "# 1. making sentence lowercase, removing all punctuation\n",
    "# 2. removing stopwords like [the, is, and, in ...]\n",
    "# 3. reducing vocabulary size helps reduce complexity\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "texts = ['cat sat on the mat', 'boy ran on the ramp', 'appple orange orange', 'car turned around the corner']\n",
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(texts)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c46ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# n-gram model \n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "text = [\"The cat chased the dog over the fence\"]\n",
    "bigrams = vectorizer.fit_transform(text)\n",
    "print(bigrams.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c20e43",
   "metadata": {},
   "source": [
    "### One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73a2300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0677c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sat']\n",
      " ['the']\n",
      " ['cat']\n",
      " ['soft']\n",
      " ['fluffy']\n",
      " ['and']\n",
      " ['was']\n",
      " ['dog']\n",
      " ['mat']\n",
      " ['chased']\n",
      " ['on']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_and</th>\n",
       "      <th>x0_cat</th>\n",
       "      <th>x0_chased</th>\n",
       "      <th>x0_dog</th>\n",
       "      <th>x0_fluffy</th>\n",
       "      <th>x0_mat</th>\n",
       "      <th>x0_on</th>\n",
       "      <th>x0_sat</th>\n",
       "      <th>x0_soft</th>\n",
       "      <th>x0_the</th>\n",
       "      <th>x0_was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fluffy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chased</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x0_and  x0_cat  x0_chased  x0_dog  x0_fluffy  x0_mat  x0_on  x0_sat  \\\n",
       "sat        0.0     0.0        0.0     0.0        0.0     0.0    0.0     1.0   \n",
       "the        0.0     0.0        0.0     0.0        0.0     0.0    0.0     0.0   \n",
       "cat        0.0     1.0        0.0     0.0        0.0     0.0    0.0     0.0   \n",
       "soft       0.0     0.0        0.0     0.0        0.0     0.0    0.0     0.0   \n",
       "fluffy     0.0     0.0        0.0     0.0        1.0     0.0    0.0     0.0   \n",
       "and        1.0     0.0        0.0     0.0        0.0     0.0    0.0     0.0   \n",
       "was        0.0     0.0        0.0     0.0        0.0     0.0    0.0     0.0   \n",
       "dog        0.0     0.0        0.0     1.0        0.0     0.0    0.0     0.0   \n",
       "mat        0.0     0.0        0.0     0.0        0.0     1.0    0.0     0.0   \n",
       "chased     0.0     0.0        1.0     0.0        0.0     0.0    0.0     0.0   \n",
       "on         0.0     0.0        0.0     0.0        0.0     0.0    1.0     0.0   \n",
       "\n",
       "        x0_soft  x0_the  x0_was  \n",
       "sat         0.0     0.0     0.0  \n",
       "the         0.0     1.0     0.0  \n",
       "cat         0.0     0.0     0.0  \n",
       "soft        1.0     0.0     0.0  \n",
       "fluffy      0.0     0.0     0.0  \n",
       "and         0.0     0.0     0.0  \n",
       "was         0.0     0.0     1.0  \n",
       "dog         0.0     0.0     0.0  \n",
       "mat         0.0     0.0     0.0  \n",
       "chased      0.0     0.0     0.0  \n",
       "on          0.0     0.0     0.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of using OneHotEncoder for categorical encoding\n",
    "\n",
    "sentences = [\n",
    "    'The cat sat on the mat.',\n",
    "    'The dog chased the cat.',\n",
    "    'The mat was soft and fluffy.'\n",
    "]\n",
    "\n",
    "unique_words = set()\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        unique_words.add(word.lower().replace('.', ''))\n",
    "\n",
    "unique_words = np.array(np.array(list(unique_words)).reshape(-1, 1))\n",
    "\n",
    "print(unique_words)\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_encoded = encoder.fit_transform(unique_words)\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(), index=unique_words.flatten())\n",
    "one_hot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6f46ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ebd7612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], shape=(2, 1000))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "one_hot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33830f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27d44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7ab5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74daaf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "imdb_dir = './data/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62cb063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just saw the movie in theater. The movie has very few good points to talk about. Kareena's beauty and a couple of songs may be. Thats it. The movie is a complete disappointment in all areas. Anyone associated with the movie will be disappointed, even Mumbai Indians too (just now Chennai has made it to semi-final). <br /><br />But the worst I feel about the movie is the action scenes. Now days Bollywood is trying to copy action scenes from Hollywood. But they forget that Hollywood directors takes a lot effect to make it look like real. But unfortunately Bollywood directors do not have that much of time. They spend their time on songs and publicity of the movie. Now such too stupid action scenes may work in South as the audience just pay to watch their favorite actor killing bunch of people. But in Bollywood this is certainly not going to work. All the action scenes I wish I could have forwarded. At the end even some Chinese people appear from nowhere to beat Akshay Kumar. This is height of stupidity. Audience is not paying to watch such stupidity. I think Bollywood now should forget about the action movies. They cant make it. The last good action I have seen was from \"Ghatak\" and \"Khiladiyon ka Khialdi\". The current scene in Bollywood is really sad for action movie fans like me. Does these people see their movie after completion? Can't they figure out that the slow motion action (which is done using ropes) is too unrealistic and childish? Better not to have action scenes if you cant handle it. I just want to go back to Amitabh's era where movie like Zanjeer and Deewar were having thrilling action scenes. The sound effect was not very effective in those days, but visually it is much better than current era scenes. <br /><br />This movie now should open the eyes of the Bollywood movie directors. Please don't make any more action movies, until you acquire the art of making it realistic.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "index = 35\n",
    "print(texts[index])\n",
    "print(labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c608af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb movie review sentiment prediction\n",
    "\n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d6c6f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7cb32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
