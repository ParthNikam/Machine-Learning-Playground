{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701134aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95792215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, callbacks\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4554eb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 1  # change this index to view a different test image\n",
    "img = test_images[idx]\n",
    "label = test_labels[idx]\n",
    "\n",
    "# If images have a channel dimension (e.g. (28,28,1)), squeeze it:\n",
    "if img.ndim == 3 and img.shape[-1] == 1:\n",
    "    img = img.squeeze(-1)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.title(f\"label={label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de981d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4cbe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "253313fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "672d12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e43dd395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                36928     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93322 (364.54 KB)\n",
      "Trainable params: 93322 (364.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4223ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c01cd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b674d856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9439WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "938/938 [==============================] - 23s 23ms/step - loss: 0.1789 - accuracy: 0.9440\n",
      "Epoch 2/5\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0469 - accuracy: 0.9853WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0469 - accuracy: 0.9854\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9898WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0331 - accuracy: 0.9898\n",
      "Epoch 4/5\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.0250 - accuracy: 0.9923WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "938/938 [==============================] - 20s 22ms/step - loss: 0.0250 - accuracy: 0.9923\n",
      "Epoch 5/5\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0196 - accuracy: 0.9941WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0196 - accuracy: 0.9941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14b1da82050>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, callbacks=[callbacks.EarlyStopping(patience=3)], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11941619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6245b56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n",
      "predicted label: 4\n",
      "probability: 0.9118717312812805\n"
     ]
    }
   ],
   "source": [
    "def load_image_for_mnist(path, target_size=(28, 28)):\n",
    "    img = Image.open(path).convert('L') # convert image into grayscale 'L' mode\n",
    "    img = img.resize(target_size, Image.BILINEAR) # resize image\n",
    "    arr = np.array(img).astype('float32') / 255.0 # convert the image into a numpy array\n",
    "    arr = 1.0 - arr # invert the colors -> white text on black background\n",
    "    arr = arr.reshape(1, target_size[0], target_size[1], 1) # input shape for the model \n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "path = \"./digit.png\"\n",
    "x = load_image_for_mnist(path)\n",
    "probs = model.predict(x)\n",
    "pred_label = np.argmax(probs, axis=1)[0]\n",
    "print(f\"predicted label: {pred_label}\")\n",
    "print(f\"probability: {max(probs[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6a93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
